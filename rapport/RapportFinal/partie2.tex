\chapter{Analyse Statique : audit d'OpenSSL}

\section{But de cette partie}
Nous nous sommes concentrés sur les parties qui peuvent être critiques : 
\begin{description}
	\item [l'entropie :] un des problèmes les plus épineux lors de la génération de clef;
	\item [la génération des clefs : ] c'est sur elles que reposent une bonne partie de la sécurité. Si leur secret venait à être découvert, les fonctions de chiffrement ne servent plus à rien (il sont en théorie connus et éprouvés);
	\item [les chiffrements et leur protocole :] régulièrement, des failles sont trouvées dans ces protocoles, et les avancées technologiques poussent aussi à rendre des algorithme obsolètes (augmentation de la puissance de calcul des machines);
	\item [les signatures et leurs authentifications : ] parce que les certificats et les signatures électroniques sont bien plus présents qu'on ne pourrait le croire. Le mécanisme est souvent transparent à l'utilisateur, mais les certificats sont devenus indispensables;
	\item [les protocoles SSL et TLS : ] OpenSSL est basé directement sur le protocole SSL puis ensuite sur celui de TLS.\\
\end{description}
Cette deuxième partie est un très court résumé du rapport d'audit réalisé pendant le projet. Pour de plus amples informations, nous vous conseillons de le consulter.


\section{Entropie}
\subsection{Introduction}
Cette partie explicite les notions d'entropie nécessaires pour la définition d'aléatoire de certains programmes. Il décrit aussi en particulier les sources de génération de bits aléatoires et leurs tests liés. \\



Trois axes principaux sont nécessaires à la mise en place d'un générateur cryptographique aléatoire de bits : 
\begin{itemize}
\item Une source de bits aléatoires (source d'entropie)
\item Un algorithme pour accumuler ces bits reçus et les faire suivre vers l'application en nécessitant.
\item Une méthode appropriée pour combiner ces deux premiers composants\\
\end{itemize}


\subsection{Estimation de l'entropie générée par la source}
Il est tout d'abord important de vérifier que la source d'entropie choisie produit suffisamment d'entropie, à un taux égalant voire dépassant une borne fixée. Pour ce faire, il faut définir avec précision la quantité d'entropie générée par la source. Il est de plus important de considérer les différents comportements des composants de la source, afin d'éliminer les interactions qu'ils peut y avoir entre les composants. En effet, ceci peut provoquer une redondance dans la génération d'entropie si cela n'est pas considéré. Étant donné une source biaisée, l'entropie générée sera conditionnée et donc plus facilement prévisible/estimable.

La source d'entropie doit donc être minutieusement choisie, sans qu'aucune interaction et conditionnement ne soit possible.

\subsection{Concept d'entropie}
\paragraph{Définition.\\}
Soit $X$ est une V.A. discrète. On définit l'\textbf{entropie} de $X$ comme suit : 
$$H(X) = - \sum_x P(X=x)*log_2(P(X=x))	 $$ 
Le logarithme est dans notre cas de base 2. L'entropie se mesure en shannons ou en bits.\\

\paragraph{Définition.\\}
On définit le \textbf{désordre} (ou incertitude) étant liée à cette expérience aléatoire. Si l'on considère l'ensemble fini des issues possibles d'une expérience $\lbrace v_1,...,v_n \rbrace$, l'entropie de l'expérience vaudra :
$$H(\epsilon) = - \sum_x P(\lbrace a_i \rbrace)*log_2(P(\lbrace a_i \rbrace))	 $$ 

\paragraph{Propriété.\\} 
On constate que l'entropie est maximale lorsque X est équi-répartie. En effet, si l'on considère n éléments de X étant équi-répartie, on retrouve notre entropie de $H(X) = log_2(n)$. \\


Ainsi, on comprend qu'une variable aléatoire apporte en moyenne un maximum d'entropie lorsqu'elle peut prendre chaque valeur avec une équiprobabilité. D'un point de vue moins théorique, on considère que plus l'entropie sera grande, plus il sera difficile de prévoir la valeur que l'on observe.

\paragraph{Min-entropy.\\}
La recommandation du NIST propose le calcul de \textit{Min-entropy} pour mesurer au pire des cas l'entropie d'une observation. \\

Soit $x_i$ un bruit de la source d'entropie. Soit $p(x_i)$ la probabilité d'obtenir $x_i$. On définit l'entropie au pire des cas telle que : 
$$\text{Min-entropy}=-log_2(max(p(x_i))$$
La probabilité d'observer $x_i$ sera donc au minimum $\frac{1}{2^\text{Min-entropy}}$.

\subsection{Source d'entropie}
\paragraph{Approche théorique.\\}
La source d'entropie est composée de 3 éléments principaux : 
\begin{itemize}
\item le \textbf{bruit source}, qui est la voûte de la sécurité du système. Ce bruit doit être non déterministe, il renvoie de façon aléatoire des bits grâce à des processus non déterministes. Le bruit ne vient pas nécessairement directement d'éléments binaires. Si ce bruit est externe, il est alors converti en données binaires. La taille des données binaires générées est fixée, de telle sorte que la sortie du bruit source soit déterminée dans un espace fixe.
\item le \textbf{composant de conditionnement}, qui permet d'augmenter ou diminuer le taux d'entropie reçu. L'algorithme de conditionnement doit être un algorithme cryptographique approuvé.
\item une \textbf{batterie de tests}, partie également intégrante du système. Des tests sont réalisés pour déterminer l'état de santé du générateur aléatoire, permettant de s'assurer que la source d'entropie fonctionne comme attendu. On considère 3 catégories de tests : 
	\begin{itemize}
	\item Les tests au démarrage sur tous les composants de la source
	\item Les tests lancés de façon continue sur le bruit généré par la source
	\item Les tests sur demande (qui peuvent prendre du temps)
	\end{itemize}
	L'objectif principal de ces tests est d'être capable d'identifier rapidement des échecs de génération d'entropie, ceci avec une forte probabilité. Il est donc important de déterminer une bonne stratégie de détermination d'échec pour chacun de ces tests.\\
\end{itemize}

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}[node distance=2cm]
\node (source) {\textbf{Source d'entropie}};
\node (bruit) [below of=source,yshift=1cm,xshift=0.5cm] {\textbf{Bruit}};
\node (donnees) [io,below of=bruit,yshift=1cm,xshift=0.5cm,pattern=dots,pattern color=orange] {Données};
\node (digit) [process, right of=donnees, xshift=4cm] {Digitalisation};
\node (cond) [process, below of=digit,text width=3cm,yshift=-0.5cm] {Conditionnement (optionnel)};
\node (bat) [process, right of=cond, xshift=4cm] {Batterie de tests}; 
\node (dec) [decision, below of=cond,yshift=-0.5cm] {Sortie}; 
\node (appli) [process, below of=dec, yshift=-1cm] {Application};

\begin{pgfonlayer}{background}
\node[punkt, fit=(donnees)(digit)(cond)(bat)(dec)(bruit)(source), fill=yellow!5] (groupclient) {};
\end{pgfonlayer}

\begin{pgfonlayer}{background}
\node[punkt, fit=(donnees)(digit)(bruit), fill=yellow!20] (groupclient) {};
\end{pgfonlayer}

\draw [arrow] (donnees) -- (digit);
\draw [arrow] (digit) -- node[anchor=east] {out1}(cond);
\draw [arrow] (cond) -- node[anchor=east] {out2} (dec);
\draw [arrow] (digit) -| node[anchor=south] {out1} (bat);
\draw [arrow] (cond) -- node[anchor=south] {out2} (bat);
\draw [arrow] (bat) |- node[anchor=north] {true/false} (dec);
\draw [arrow] (dec) -- node[anchor=east] {output/error} (appli);
\end{tikzpicture}
\end{center}
\caption[Composants d'une source d'entropie]{Composants d'une source d'entropie. \texttt{out1} est une chaîne binaire de taille quelconque et \texttt{out2} est une chaîne binaire conditionnée de taille fixe. }
\end{figure}

\subsection{Standards}
	\subsubsection{RFC 4086}

Nos machines utilisent ce qu'on appelle des PRNG (Pseudo Random Number Generator), qui sont des algorithmes qui génèrent une séquence de nombres s'apparentant à de l'aléatoire. En réalité rien n'est aléatoire car tout est déterminé par des valeurs initiales (état du PRNG) et des contextes d'utilisation.\\
			
Un bon PRNG se doit d'avoir une très forte entropie (proche de un), afin d'éviter de délivrer de l'information.

Comme l'entropie est fournie majoritairement (si ce n'est totalement) par l'OS, chacun présente ses forces et ses faiblesses. Dans le rapport d'audit sont détaillés les différentes méthodes de différents OS.\\
		
Nous nous basons sur la RFC 4086 \cite{rfc4086}: \textit{Randomness requirements for security} pour le choix des PRNG selon les différents systèmes.	
	
	\subsubsection{FIPS 140}
Le FIPS 140 (\textit{Ferderal Information Processing Standards}) est un standard du gouvernement américain spécifique aux modules cryptographiques déployés par des éléments du gouvernement. Il inclue notamment des standards de tests qui permettent de décrire la qualité et/ou valider des générateurs d'entropie ou générateurs pseudo-aléatoires. La version la plus récente est le FIPS 140-2, qui décrit plusieurs niveaux de tests.\\
		
		
		Les test du FIPS 140-1 permettent de s'assurer que les sources d'entropies produisent suffisamment de "bonnes" données, pourvu que les sources d'entropies n'utilisent pas quelques opérations cryptographiques internes. Si une source d'entropie en utilise, alors elle réussira les tests avec quasi-certitude, même si la source d'entropie est faible. De ce fait, ces jeux de tests ne sont pas bons pour tester des générateurs de nombres pseudo-aléatoires cryptographiques, car ceux-ci passeront facilement les tests même si les générateurs d'entropie sont faibles. Par exemple, si l'on hache une suite d'entiers (par pas de 1), les tests seront tous validés bien qu'ils n'auront pas été tirés aléatoirement, ceci en vertu de la fonction de hachage. Pourtant, la donnée est hautement prédictible.\\
		
		
		Les tests du FIPS 140-2 ne sont également pas très efficaces, et permettent seulement de détecter si un matériel commence à produire un motif répété. Ils consistent à comparer différentes sorties consécutives d'un générateur. Ainsi, si le "générateur aléatoire" consiste à produire une simple incrémentation de nombres, les tests passeront sans problème.\\
		
		
		Il est donc conseillé d'utiliser les tests du FIPS 140-1 et 140-2 pour vérifier uniquement si la source d'entropie produit de bonnes données, au démarrage et périodiquement lorsque c'est possible.
		
		\subsubsection{Quelques failles}
			\subsubsubsection{Le cas Debian 4.0 et OpenSSL 0.9.8}
C'est l'une des failles qui a fait grand bruit ces dernières années.\\
Le 13 mai 2008, Luciano Bello a découvert une faille critique du paquet d’OpenSSL sur les systèmes Debian. Un mainteneur Debian souhaitant corriger quelques bugs aurait malencontreusement supprimé une grosse source d’entropie lors de la génération des clefs. Il ne restait plus que le PID comme source d’entropie. Comme celui-ci ne pouvait dépasser 32 768 (qui est le PID maximal par défaut atteignable), l’espace des clefs a été restreint à 264 148 clefs distinctes.\\

Il est donc fortement recommandé de mettre à jour sa version d'OpenSSL vers une version stable où le bug a été corrigé, puis de générer de nouvelles clefs de chiffrement, et de révoquer les certificats corrompus. Il reste malheureusement beaucoup de certificats touchés par la faille, la plupart été valides jusqu'en 2020-2030.

			\subsubsubsection{Le cas MinuxMintDebianEdition sous Android}
\paragraph{Faille.\\}
Récemment, en août 2013 précisément, un patch de sécurité pour les systèmes Android utilisant la version LinuxMintDebianEdition/OpenSSL, dévoile une réparation du générateur de nombres pseudo-aléatoires (PRNG) qui ne donnait pas suffisamment d’entropie.\\


Le patch indique que le PRNG de cette version d’OpenSSL utilise dorénavant une combinaison de données plus ou moins prévisibles associées à l’entropie générées par /dev/urandom. Mais sachant que le PRNG d’OpenSSL utilise lui-même /dev/urandom, on a du mal à comprendre pourquoi en rajouter davantage.\\


Eric Wong et Martin Boßlet apportent la solution sur leur site. Ils montrent que l’erreur provient d’un bug "à la Debian", une simple ligne diffère de la version officielle d’OpenSSL (utilisant SecureRandom) à celle de OpenSSL : Random se situant dans la fonction ssleay\_rand\_Toutefois. bytes, la conséquence n’est pas aussi lourde que celle de Debian, tout d’abord parce que le système Android est rarement utilisé
pour du chiffrement de données sensibles, et une attaque par prédiction bien que plus rapide qu’une attaque par brute-force reste infaisable. Mais l’erreur est quand même là.\\

\paragraph{Recommandations.\\}
Il est alors recommandé à court terme d'ajouter plus d'ntropie à notre PRNG sous Android, soit manuellement comme le script Ruby d'Eric Wong, ou avec des outils cités plus haut dans ce document.Sinon, le patch d'OpenSSL répare également l'erreur en rajoutant plus d'aléatoire, bien qu'une recommandation officielle serait l bienvenue, ainsi qu'une meilleure documentation.

			\subsubsubsection{NetBSD 6.0 et OpenSSH}
\paragraph{Faille.\\}
Autre faille du même genre sur les systèmes NetBSD 6.0 concernant OpenSSH/OpenSSL et datant de mars 2013. L’erreur provient d’une insuffisance d’entropie dans le PRNG, qui ne tient que sur 32 ou 64 bits (la taille d’un sizeof(int)). Un attaquant peut brute-forcer une clef générée par ce PRNG. Il est probable que les dégâts soient bien moins étendus que lors de l’affaire OpenSSL Debian car les systèmes NetBSD 6.0 sont moins fréquents.\\

Il est recommandé de passer à une version NetBSD supérieure à 5.1, et de générer de nouvelles données de chiffrement comme les clés SSH ayant étés générées avec ce noyau. Il faut également faire attention au patch de sécurité de janvier 2013 qui en tentant de régler le problème a généré une autre erreur produisant le même effet.	L'erreur ne provient pas directement du code d'OpenSSL. Mais ici, OpenSSL se contente de récupérer l'entropie fournie sans aucune vérification (fonction \texttt{RAND\_bytes()}).\\ 

\paragraph{Recommandations.\\}
Il est également recommandé d'utiliser \texttt{/dev/random} plutôt que \texttt{/dev/urandom} pour avoir une bonne entropie. On souligne quand même le fait que OpenSSL ne contrôle pas son entropie, si l'entropie du système est quasi-nulle les clefs sont tout de même générées. Un avertissement auprès de l'utilisateur serait un minimum.\\


De manière plus générale, le NIST propose des recommandations à plusieurs niveaux concernant la conception de la source d'entropie. Il est possible d'en trouver dans le rapport du NIST SP800-90B.
			

\section{Génération des clefs}
La génération de clefs est une notion fondamentale de cryptographie. En effet, les données sont protégées grâce à des algorithmes (ou des méthodes cryptographiques) et des clefs cryptographiques. Celles-ci sont notamment utilisées pour le chiffrement et le déchiffrement de données.\\
Relativement aux deux types de cryptographie, on compte deux types de clefs : 
\begin{itemize}
\item les clefs symétriques pour le chiffrement symétrique;
\item les clefs publiques/privées pour le chiffrement asymétriques.\\
\end{itemize}

On se considérera dans ce chapitre dans un milieu optimal : 
\begin{enumerate}
\item l'algorithme cryptographique considéré est optimal;
\item la génération de bits aléatoire est optimale.
\end{enumerate}


\subsection{Clefs d'algorithmes symétriques}
Les algorithmes symétriques utilisent pour leur chiffrement une clef symétrique qui est partagée par chacun des correspondants. 

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}[node distance=2cm]
\node 	(titre) 		[] 		{\textbf{Echange de message par chiffrement symétrique}};
\node 	(k1) 	[below of=titre,xshift=-3cm,yshift=1cm] {k};
\node 	(k2) 	[right of=k1,xshift=5cm] {k};
\node 	(E) 		[processS,below of=k1] {E (Alice)};
\node 	(D) 		[processS,below of=k2] {D (Bob)};
\node 	(m1)		[left of=E] {m};
\node 	(m2)		[right of=D] {m};
\begin{pgfonlayer}{background}
\node[punkt, fit=(titre)(E)(D)(k1)(k2)(m1)(m2), fill=yellow!5] (groupclient) {};
\end{pgfonlayer}


\draw	[arrow]	(k1) -- (E);
\draw	[arrow]	(k2) -- (D);
\draw	[arrow]	(D) -- (m2);
\draw	[arrow]	(m1) -- (E);
\draw	[arrow]	(E) -- node[anchor=north] {$E_k(m)=c$} (D);
\end{tikzpicture}
\end{center}
\caption[Chiffrement symétrique]{Chiffrement symétrique - Alice envoie un message $c$ chiffré à Bob qui le déchiffre}
\label{sym}
\end{figure}


\subsection{Clefs d'algorithmes asymétriques}

Les clefs d'algorithmes symétriques sont de deux catégories, les clefs privées et les clefs publiques. Les clefs publiques sont disponibles pour tous, par demande à l'utilisateur, ou bien souvent par certificat. 

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}[node distance=2cm]
\node 	(titre) 		[] 		{\textbf{Echange de message par chiffrement asymétrique}};
\node 	(k1) 	[below of=titre,xshift=-3cm,yshift=1cm] {};
\node 	(k2) 	[right of=k1,xshift=5cm] {$k_B'$};
\node 	(E) 		[processS,below of=k1] {E (Alice)};
\node 	(D) 		[processS,below of=k2] {D (Bob)};
\node 	(m1)		[left of=E] {m};
\node 	(m2)		[right of=D] {m};
\node 	(key1) 	[below of=E] {$k_A$ (publique), $k_A'$ (privée)};
\node 	(key2) 	[below of=D] {$k_B$ (publique), $k_B'$ (privée)};

\begin{pgfonlayer}{background}
\node[punkt, fit=(titre)(E)(D)(k1)(k2)(m1)(m2)(key1)(key2), fill=yellow!5] (groupclient) {};
\end{pgfonlayer}


\draw	[arrow]	(k2) -- (D);
\draw	[arrow]	(D) -- (m2);
\draw	[arrow]	(m1) -- (E);
\draw	[arrow]	(E) --  node[anchor=north] {$E_{k_B}(m)=c$} (D);

\end{tikzpicture}
\end{center}
\caption[Chiffrement asymétrique]{Chiffrement asymétrique - Alice envoie un message $c$ chiffré à Bob avec sa clef publique qui le déchiffre}
\label{asym}
\end{figure}

Comme montré en figure \ref{asym}, Alice chiffre avec la clef publique de Bob un message, qui lui-même déchiffre le message avec la clef privée.

\subsection{Méthode de génération et sécurité}


Les systèmes utilisent des entiers comme clef. Les clefs sont générées en utilisant des générateurs de nombres aléatoires (RNG) ou des générateurs de nombres pseudo aléatoires (PRNG). 

\subsubsection{Génération de clés quelconques}
Un générateur de bits aléatoires (RGB) approuvé est utilisé pour générer la clef cryptographique. Le RGB doit fournir une entropie complète ou suffisante aux exigences de sécurité du système. Les RGB peuvent  permettre de générer complètement et "directement" la clef. C'est le cas pour les clefs privées des algorithmes AES ou DSA.
Les RGB peuvent aussi être utilisés comme \textit{seed} pour générer des clefs, suivants des critères spécifiques. C'est le cas pour RSA, où habituellement le \textit{seed} est utilisé comme point de départ pour trouver un nombre premier suivant les critères du FIPS 186-3.

\subsubsection{Génération de paires de clefs asymétriques}
Les algorithmes asymétriques demandent la création de paires de clefs asymétriques (privée/publique), dont chaque clef est associée à une seule entité, son possesseur. Les clefs sont généralement générées par leur possesseur direct, ou par une autorité de confiance qui fournit la clef au possesseur de façon sûre.\\
Il convient au moment de la génération de bien considérer deux paramètres principaux : 
\begin{enumerate}
\item l'espace de clefs, qui doit être suffisamment grand pour éviter les doublons et les cycles;
\item générer les premiers de façon la plus aléatoire possible. Ceci nécessite un bon choix de stratégie d'identification de premiers. On peut par exemple utiliser des algorithmes permettant de détecter le prochain premier suivant un entier tiré aléatoirement. 
\end{enumerate}

\subsection{Exemples de faille}
\subsubsection{Le générateur Diffie-Hellman}

		Voici l'algorithme de génération de g, d'après la RFC 2631 datant de 1999 et dérivé de la FIPS-186 : 
		\begin{itemize}
		\item 1- Soit $j = (p - 1)/q$.
		\item 2- Choisir $h \in \mathbb{N}$, tel que $1 < h < p - 1$
		\item 3- Calculer $g = h^j \mod p$
		\item 4- Si $g = 1$ recommencer l'étape 2\\
		\end{itemize}
	
		Mais depuis 2006, on peut lire comme recommandation dans la RFC 4419 (pour une utilisation SSH) : "\textit{It is recommended to use 2 as generator, because it improves	efficiency in multiplication performance.  It is usable even when itis not a primitive root, as it still covers half of the space of possible residues.}"\\
	
		Lorsque nous avons étudié le code de Diffie-Hellman dans OpenSSL, nous nous sommes penchés sur un choix plutôt étrange. La valeur du générateur est toujours fixé à 2 ou à 5. \\
	
		Le générateur de Diffie-Hellman n'étant pas une racine primitive dans $\mathbb{Z}/\mathbb{Z}_p$, les conséquences sont :
		\begin{itemize}
		\item l'espace des clefs possibles est fortement réduit (Si $g=2 \implies$ espace divisé par deux);
		\item deux clefs privées distinctes pourront avoir une clé publique commune;
		\item la méthode de cryptanalyse Baby-step Giant-step peut s'en trouver 
		facilité.\\
		\end{itemize}
	
		Évidemment, ce choix n'est pas une faille en soit, il n'est juste pas optimal et résulte d'un bon compromis entre vitesse et sécurité. Pour une sécurité optimale, il est conseillé de choisir un générateur qui soit une racine primitive, pour être certain que personne ne puisse signer, déchiffrer des messages à votre place!\\
		
\paragraph{Norme.\\}
Ainsi selon la norme RFC-4419, le choix du générateur peut se résumer à un petit générateur qui ne serait pas une racine primitive. La criticité du risque est très grande, car si deux personnes possèdent la même clef publique pour deux clefs privées distinctes, ils pourront alors déchiffrer les messages, et signer à la place de l'autre. Mais, la probabilité d'une telle collision est quasiment nulle, il est simplement deux fois plus efficace de générer deux clés privées possédant la même clef publique, que de retrouver la clef privée en brute force.\\
OpenSSL laisse en plus le choix au développeur de choisir une racine primitive comme générateur, pour des cas d'extrême sécurité.

\subsubsection{Diffie-Hellman Ephémère en mode FIPS}
\paragraph{Faille.\\}
Une faille plus grave concerne le mode FIPS (Federal Information Processing Standard) d'OpenSSL, qui peut être compilé avec la commande \texttt{./config fipscanisterbuild}". En effet, un attaquant situé entre le client et le serveur connaissant la clef secrète du serveur peut déchiffrer une session SSL/TLS. \\
	
		L'algorithme EDH/DHE (Diffie-Hellman Éphémère) permet de calculer une nouvelle clef connue uniquement du client et du serveur, donc l'attaquant intermédiaire ne peut plus déchiffrer la session. Cependant, en mode FIPS, OpenSSL ne rejette pas les paramètres P/Q faibles pour EDH/DHE. Lorsque OpenSSL est compilé en mode FIPS, un attaquant en Man-in-the-middle peut donc forcer la génération d'un secret Diffie Hellman prédictible, en modifiant par exemple le traffic réseau..

		La faille en elle même n'est pas suffisante pour faire l'attaque, elle requiert également une implémentation SSL faible. \\
		
\paragraph{Recommandations.\\}
		Il est tout d'abord recommandé de passer à la version OpenSSL supérieur, sinon de désactiver le mode FIPS, ou encore de configurer la \textit{ciphersuite} afin de ne pas permettre au serveur d'utiliser DH comme algorithme d'échange de clefs.


\section{Chiffrement et protocoles}

\subsection{Définitions et contexte}

Nous allons voir qu'une combinaison entre un mode de chiffrement et un protocole de chiffrement peut générer des failles, souvent grave (i.e vols de mots de passes), que le padding est également une donnée sensible et qu'il faut donc le choisir intelligemment, et qu'une mauvaise documentation ou un mauvais paramétrage peuvent entraîner des failles au niveau protocolaire.

\subsubsection{Les "Manger's attack" sur RSA-OAEP}
\paragraph{OAEP : \textit{Optimal Asymmetric Encryption Padding}. \\}
Dans les chiffrements par blocs, cela nécessite généralement que tous les blocs soient d'une taille précise. Or ce n'est pas toujours le cas. Pour cela, on rajoute des bits de bourrage (padding).\\
OAEP est un schéma de remplissage, généralement utilisé avec RSA (en prétraitement). Il a été introduit en 1994 par Mihir Bellare et Phil Rogaway1. L'OAEP est une forme de réseau de Feistel qui nécessite une source d'aléa ainsi que deux fonctions de hachage.\\
RSA-OEAP peut être prouvé sûr dans un modèle théorique idéalisé, celui de l'oracle aléatoire. Il est recommandé par les PKCS.\\
OAEP a deux buts :
\begin{itemize}
\item insérer un élément d'aléatoire qui permet de passer d'un schéma déterministe à un schéma non déterministe (le même message clair chiffré deux fois avec la même clef et le même algorithme n'aura pas le même message chiffré.);
\item prévenir un déchiffrement partiel en s'assurant que l'attaquant ne peut retrouver une portion du texte clair sans être capable d'inverser la fonction trapdoor (par exemple la factorisation de deux grands nombres premiers : il est facile de multiplier, mais quand on n'a que le produit il est très difficile de retrouver les facteurs).\\
\end{itemize}
Il n'est pas prouvé sûr pour une attaque IND-CCA (attaque à texte chiffré seulement). Victor Shoup a démontré qu'il n'existe pas de preuve générale.
Il a montré que dans un cas IND-CCA, quelqu'un qui sait comme inverser partiellement une primitive d'insertion mais ne sait pas comment l'inverser complètement, pourrait bien être en mesure de casser le système. Par exemple, on peut imaginer quelqu'un qui peut attaquer RSAES-OAEP si on sait comment retrouver tous les octets exceptés les 20 premiers d'un entier généré aléatoirement chiffré avec RSAEP. Un tel attaquant n'a pas besoin d'être capable d'inverser entièrement RSAEP (RSA Encryption Protocole), parce qu'il n'utilise pas les 20 premiers octets dans son attaque.\\


\paragraph{Norme.\\}
Note de la PCKS\# 1 (RFC 3447) : Il faut faire attention qu'un adversaire ne puisse distinguer les différentes erreurs renvoyées, que ce soit par un message d'erreur, ou un temps de réponse différent, ou, plus généralement, apprendre une information partielle à propos du message en clair \texttt{EM}. Sinon un adversaire peut être en mesure d'obtenir des informations utiles sur le déchiffrement du texte chiffré \texttt{C}, conduisant à une attaque à chiffré choisi telle que celle observée par Manger.

\paragraph{Description de la faille.\\}
RSA-OAEP peut être soumis à une attaque nommée "Mangers Attack" selon son implantation. OpenSSL semble être vulnérable à une attaque de ce type, à base de "prédictions" par injections de fautes. La vulnérabilité semble être très récente puisqu'elle fonctionne sous OpenSSL 1.0.0.\\

\paragraph{Correction.\\}
Le padding OAEP devait palier le problème d'insécurité que causait le padding PKCS\#1 v1.5 (attaque à chiffré choisi). OpenSSL a tout de même pris en compte cette vulnérabilité et a placé des contres-mesures efficaces. La Technische Universität Darmstadt (Allemagne) explique en détails comment sont implémentées ces contres-mesures et montre que dans certains cas l'attaque reste possible. Enfin, elle apporte ses propres contre-mesures.\\

On peut noter que plusieurs librairies sont vulnérables à une attaque de Manger (voir paragraphe suivant) qui consiste à contrôler la taille des paramètres à hacher, mais que l'implantation de RSA-OAEP d'OpenSSL ne le permet pas. La raison est que le décodage OAEP est linéaire quelque soit la taille des paramètres et les erreurs survenues. Il semble également y avoir un problème avec l'OAEP\_padding sur le chiffrement RSA. Bill Nickless recommande l'utilisation de PKCS\_padding.\\

\paragraph{Recommandation.\\}
Il n'y a pas vraiment de quoi s'alarmer, cette attaque est en pratique infaisable sur un serveur car il y a suffisamment de variations de délais (différences de CPU, opérations multi-tâches, connexions réseaux, etc...) pour éviter une attaque par timing. Cependant, sur des systèmes embarqués l'attaque peut être réalisable, et il serait plus prudent de palier ce problème.

\subsubsection{Chiffrement SSLv3 ou TLS 1.0 en mode CBC}
\paragraph{Faille.\\}
En Septembre 2011, une attaque en \textit{man in the middle} très efficace a vu le jour contre les protocoles SSLv3 et TLS 1.0. . L'attaque est à clair choisi. Le but étant d'insérer des morceaux de texte clair grâce au navigateur dans la requête chiffrée avec ces protocoles, ceci afin de récupérer les cookies de session.\\

La technique est basique, un individu enregistre plusieurs cookies de session auprès de divers sites officiels (banques, messageries, etc...). Puis, il clique malencontreusement sur du code Java malveillant (publicité, image, etc...). et l'attaque se déroule automatiquement. L'ensemble des cookies est envoyé au serveur malveillant qui n'a plus qu'à déchiffrer les clés de session.\\

La cause viendrait du mode de chiffrement choisi : CBC. SSL/TLS est un protocole qui chiffre un canal de communication. De ce fait il ne chiffre pas un fichier unique, mais une série d'enregistrements. Il y a deux façons d'utiliser le mode CBC dans ce cas précis :
\begin{itemize}
\item prendre chacun de ces enregistrements indépendamment des autres. Générer un nouveau vecteur d'initialisation à chaque fois;
\item traiter ces enregistrements comme un seul objet en les concaténant. Le vecteur d'initialisation est donc choisi aléatoirement pour le premier enregistrement et pour les autres, il aura pour valeur le dernier bloc de l'enregistrement précédent.\\

\end{itemize}

\paragraph{Recommandations.\\}
SSLv3 et TLS 1.0 utilisent ce deuxième choix, cela soulève un lourd problème de sécurité. En 2004, Moeller trouve une méthode pour exploiter ce mauvais choix afin de récupérer des morceaux de textes clairs. Il y a certes une faille immense, mais peu exploitable. Les grandes entreprises savent (normalement) qu'il ne faut pas utiliser le mode CBC pour du chiffrement SSL/TLS. Et, dans tous les cas, plusieurs navigateurs ne permettent pas ce type d'attaque (c'est le cas de Chrome par exemple).\\

La faille existe tant que l'association de ces protocoles avec le mode de chiffrement CBC existe. Même si l'attaque est infaisable sur les navigateurs les plus répandus (Chrome, Firefox, IE, Safari, ...), OpenSSL devrait pouvoir interdire cette association, et ne pas laisser le travail aux navigateurs. Mais rien n'empêche l'utilisation de ce chiffrement par un navigateur plus léger, nous pourrons tester cette vulnérabilité lors de notre partie 3 si nous trouvons un navigateur acceptant ce type de chiffrement.

\subsubsection{Non-validation des certificats SSL}

\paragraph{Norme.\\}
La RFC 5246 stipule que le serveur doit toujours envoyer un message certificat dès lors que la méthode d'échange des clefs a été acceptée. 
Le certificat DOIT être approprié à la suite des chiffrements utilisés pour les échanges de clefs.\\

\paragraph{Faille.\\}
Six chercheurs des universités de Stanford et d'Austin au Texas, analysent une attaque en Man in the Middle autour des certificats SSL sans utilisation d'un navigateur. Le titre est sans appel "Le code le plus dangereux du monde".\\


SSL doit permettre d'être sécurisé en toute circonstance, que le cache DNS soit empoisonné, que les attaquants contrôlent les points d'accès et les routeurs, etc. . Il assure théoriquement trois grands principes de la cryptologie : la confidentialité, l'intégrité et l'authentification. Nous connaissons certaines failles au niveau du navigateur et de l'implantation SSL (voir ci-dessus). Mais il existe également d'autres cas d'utilisation du protocole SSL. Par exemple:
\begin{itemize}
\item Administration à distance basé sur le cloud, stockage sécurisé sur le cloud en local.
\item Transmissions de données sensibles (ex: e-commerce)
\item Services en ligne comme les messageries électroniques
\item Authentification via applications mobiles comme Android et iOS\\
\end{itemize}

L'étude montre que la validation des certificats SSL est casée sur plusieurs applications et librairies dont :
\begin{itemize}
\item OpenSSL
\item JSSE
\item CryptoAPI
\item NSS
\item GnuTLS
\item etc...\\
\end{itemize}

En fait, un attaquant en \textit{Man In The Middle} peut intercepter le secret entre un client et un serveur utilisant une connexion SSL. Il peut ainsi récupérer des numéros de carte bancaire, avoir accès à une messagerie, récupérer des mots de passes, etc... La cause principale vient du fait que les développeurs retouchent les librairies cryptographiques à leur façon. En voulant réparer un bug ou en souhaitant rendre SSL compatible avec leurs API, ils injectent de nouvelles vulnérabilités. De plus, l'application est souvent propriétaire et payante ce qui rend le déboggage difficile.\\


Que ce soit accidentel ou intentionnel, l'une des conséquences les plus graves est la non-validation de certificat sur des contexte où la sécurité est primordiale (e.g. payement en ligne). La faute ne revient pas directement au code d'OpenSSL, mais à une mauvaise utilisation des différentes fonctions et options.\\


\subsubsubsection{Difficultés du code OpenSSL}


OpenSSL ne déroge pas à la règle.\\
Voici quelques vulnérabilités du code :
\begin{itemize}
\item Les contraintes de nom x509 ne sont pas correctement validés.
\item Les applications DOIVENT fournir elles même leur code de vérification de nom d'hôte. Or, des protocoles comme HTTPS, LDAP ont chacun leurs propres notions de validations. Ainsi, Apache Libcloud utilise les librairie Python eux-même utilisant des commandes OpenSSL. Et sa méthode de vérification du nom d'hôte comporte des vulnérabilités pouvant causer des attaques en man in the middle (e.g "google.com" et "oogle.com" vérifie la même expression régulière)
\item Un programme utilisant OpenSSL peut exécuter la fonction \texttt{SSL\_connect} pour le handshake SSL. Bien que certaines erreurs de validation soient signalées par \texttt{SSL\_connect}, d'autres ne peuvent être vérifier qu'en appelant la fonction \texttt{SSL\_get\_verify\_result}, alors que \texttt{SSL\_connect} se contente de retourner "OK".
\end{itemize}

\subsubsubsection{Exemple : Trillian}

Trillian est une messagerie cliente instantanée reliée à OpenSSL pour la sécurisation de l'établissement de connexion. Par défaut OpenSSL ne soulève pas d'exception en cas de certificat auto-signé ou de non-confiance auprès de la chaîne de vérification. A la place, il envoie un drapeau. De plus, il ne vérifie jamais le nom d'hôte. Si l'application appelle la fonction \texttt{SSL\_CTX\_set} pour initialiser le drapeau \texttt{SSL\_VERIFY\_PEER}, alors \texttt{SSL\_connect} se ferme et affiche un message d'erreur lorsque le certificat n'est pas valide. Mais Trillian n'initialise jamais ce drapeau. Par conséquent, \texttt{SSL\_connect} va retourner 1 et le statut de la validation du certificat peut être connu en appelant la fonction \texttt{SSL\_get\_verify\_result}. Encore une fois, Trillian n'appelle pas cette fonction. Les conséquences sont très lourdes : vols de mots de passes, compromissions de services, révélations des paramètres de sécurité, etc...\\


L'étude montre que l'attaque est possible sur la version 5.1.0.19 et antérieure de Trillian.

\subsection{Conclusion}

Les chercheurs nous donnent alors plusieurs leçons à retenir, dont voici quelques points :
\begin{itemize}
\item Premièrement, les vulnérabilités doivent être trouvées et réparées lors des phases de tests. Certaines se trouvent très facilement si les procédures de tests sont bien réalisées.
\item Deuxièmement, la plupart des librairies SSL ne sont pas \textbf{sûres par défaut}, laissant le choix de la sécurité aux applications de plus haut niveau avec choix des options, choix de la vérification de l'hôte, choix d'interprétation des résultats.
\item Troisièmement, même les librairies SSL sûrs par défaut peuvent être mal utilisées par des développeurs changeant les paramètres par défaut par des paramètres non sécurisés. La cause peut venir d'une \textbf{mauvaise documentation} ou d'une mauvaise formalisation de la part de l'API. Les API devraient entre autre proposer des abstractions de haut niveau pour les développeurs comme des tunnels d'authentification, plutôt que de les laisser traiter des détails de bas niveau comme la vérification du nom d'hôte.\\
\end{itemize}

Nous conseillons surtout une meilleure documentation d'OpenSSL, et des rapports d'erreurs d'interfaces plus simples et plus consistants afin d'éviter les erreurs d'interprétation. L'idée des chercheurs de proposer des abstractions de haut niveau pour les applications semblent être une très bonne idée.


\section{Signature et authentification}

\subsection{Définitions et contexte}

Une signature électronique utilise le concept de la traditionnelle signature sur papier et la tourne en une empreinte électronique. Cette empreinte est un message encodé et est unique pour chaque document et chaque signataire. La signature permet alors de garantir l'authenticité du signataire pour son document. Toute modification dans le document après l'avoir signé rend la signature invalide, ce qui protège alors contre les fausses informations et la contrefaçon des signatures.\\

De ce fait, il est important de faire attention à toute les formes de vulnérabilités des signatures afin de comprendre les attaques possibles sur la contrefaçon des signatures. Cette partie est consacrée à la bonne compréhension et l'implémentation qui sont définies dans la RFC, afin de se prémunir des différentes attaques possibles. On verra, cependant, qu'il existe quand même des failles au niveau des injections, surtout lorsque la librairie dépend trop du matériel. 

\subsection{Attaque par injection de fautes sur les certificats RSA}
Dans la RFC 3447, la signature est décrite telle une primitive de signature qui produit la représentation de la signature depuis un message sous le contrôle d'une clef privée. La vérification se fait alors en récupérant la représentation du message depuis la représentation de la signature sous le contrôle de la clef publique correspondante.\\

La signature se déroule en  deux opérations qui sont la génération et la vérification. L'opération de génération consiste donc à générer une signature depuis un message avec la clef privée de l'utilisateur (signataire) et l'opération de vérification consiste à vérifier la signature en se basant sur le message en utilisant la clef publique du signataire. 
Ce schéma peut être utilisé dans de multiples applications telles que les certificats X.509.\\

L'Université du Michigan a réussi l'exploit de récupérer la clé privée d'un certificat RSA en un peu plus de 100h. L'attaque fonctionne par injection de fautes sur la méthode d'authentification. La technique est donc très poussée, mais le résultat en vaut la chandelle. L'injection de faute doit se faire sur quelques bits pour ne pas faire dysfonctionner le système tout entier. Les signatures erronées produites révéleront de l'information sur la clé privée. Avec le bon matériel et 100h d'attente, la clef peut être reforgée.\\

			La technique consite à renseigner de fausses signatures afin de vérifier les fautes avec la clef publique de la machine.\\
			
			Lorsque le système est vulnérable, OpenSSL ne le détecte pas forcément. Le risque est donc très fort, et les contre-mesures sont parfois difficiles à trouver dans les phases de tests. Toutefois, cette étude soulève un choix de programmation qui semble à première vue anodin, mais qui peut avoir de lourdes conséquences.\\

			Toutefois, il faut pouvoir contrôler la machine (en ayant un accès au BIOS par exemple) pour pouvoir exploiter cette faille car il faut pouvoir toucher directement à l'alimentation de la faille.\\

			Cependant, cette erreur n'est pas à prendre à la légère, car une attaque à base de faiseaux lumineux est en cours de développement afin de réaliser cette attaque à distance.
			
\subsection{Malformation des signatures DSA/ECDSA}
La RFC R979 définit l'utilisation de DSA (\textit{Digital Signature Algorithm}) et ECDSA (\textit{Elliptic Curve Digital Signature Algorithm}) de façon déterministe. DSA et ECDSA sont deux standards de signature électronique qui offrent l'intégrité et authenticité dans de nombreux protocoles.\\

En 2008, une vulnérabilité sur la malformation des signatures survient sur OpenSSL (re-analysé en Novembre 2012). 

		La cause vient de plusieurs fonctions implémentant la fonction EVP\_VerifyFinal() (cf. \textit{}). Elles valident de fausses signatures au lieu de retourner des erreurs, parmis les signatures corrompues peuvent se trouver :
		\begin{itemize}
		\item des signatures DSA;
		\item des signatures ECDSA.\\
		\end{itemize}

		En 2009, un cas similaire a été trouvé dans un autre protocole (NTP) avec la même fonction \texttt{EVP\_VerifyFinal}.

		La conséquence est très grave, car cette faille permet une attaque en \textit{man in the middle}, en faisant par exemple une attaque par \textit{phishing} en HTTPS où la validation de la chaîne des certificats serait valide.\\
		
		Ici, la faille persistera tant que le serveur et le client resteront à une version antérieur à OpenSSL 0.9.8j, les clefs quant à elles ne sont pas vulnérables, et peuvent être conservées. Malheureusement, le nombre de serveurs tournant sous OpenSSL 0.9.8 et versions antérieures est très élevé.\\

			Il est également recommandé aux développeurs utilisant OpenSSL de faire des audits réguliers de la fonction EVP\_VerifyFinal() pour s'assurer que les vérifications sont bonnes. Les tests étants assez simples à effectuer.


